{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kVQsCEtvkJLm"
   },
   "outputs": [],
   "source": [
    "HF_TOKEN = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "vOmliifBpUbo",
    "outputId": "c0dc20a5-11a7-4be5-d813-155235c425d5"
   },
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "\n",
    "model_dir = \"./model\"\n",
    "compute_type = \"float16\"\n",
    "audio_path = \"./data/samples/audio_sample_3.wav\"\n",
    "\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=HF_TOKEN, \n",
    "    cache_dir=model_dir)\n",
    "\n",
    "# send pipeline to GPU (when available)\n",
    "import torch\n",
    "pipeline.to(torch.device(\"cuda\"))\n",
    "\n",
    "# apply pretrained pipeline\n",
    "diarization = pipeline(audio_path)\n",
    "\n",
    "# # print the result\n",
    "# for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "#     print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CtB5cHKVvKM1",
    "outputId": "e4638fef-c721-4312-c6b1-038dfa1407d6"
   },
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import os\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = \"segments\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load the original audio\n",
    "waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "# Save each segment to a separate file\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    segment_path = os.path.join(output_dir, f\"segment_{turn.start:.1f}_{turn.end:.1f}.wav\")\n",
    "    start_sample = int(turn.start * sample_rate)\n",
    "    end_sample = int(turn.end * sample_rate)\n",
    "    torchaudio.save(segment_path, waveform[:, start_sample:end_sample], sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ahDFzQ9uzTyV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "During parameter transfer to RNNLM(\n",
      "  (embedding): Embedding(\n",
      "    (Embedding): Embedding(1000, 256)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (rnn): LSTM(\n",
      "    (rnn): LSTM(256, 2048, num_layers=2, batch_first=True)\n",
      "  )\n",
      "  (dnn): Sequential(\n",
      "    (linear): Linear(\n",
      "      (w): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (norm): LayerNorm(\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (act): LeakyReLU(negative_slope=0.01)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (out): Linear(\n",
      "    (w): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      ") loading from tmpdir/lm.ckpt, the transferred parameters did not have parameters for the key: rnn.rnn.weight_ih_l0\n",
      "During parameter transfer to RNNLM(\n",
      "  (embedding): Embedding(\n",
      "    (Embedding): Embedding(1000, 256)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (rnn): LSTM(\n",
      "    (rnn): LSTM(256, 2048, num_layers=2, batch_first=True)\n",
      "  )\n",
      "  (dnn): Sequential(\n",
      "    (linear): Linear(\n",
      "      (w): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (norm): LayerNorm(\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (act): LeakyReLU(negative_slope=0.01)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (out): Linear(\n",
      "    (w): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      ") loading from tmpdir/lm.ckpt, the transferred parameters did not have parameters for the key: rnn.rnn.weight_hh_l0\n",
      "During parameter transfer to RNNLM(\n",
      "  (embedding): Embedding(\n",
      "    (Embedding): Embedding(1000, 256)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (rnn): LSTM(\n",
      "    (rnn): LSTM(256, 2048, num_layers=2, batch_first=True)\n",
      "  )\n",
      "  (dnn): Sequential(\n",
      "    (linear): Linear(\n",
      "      (w): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (norm): LayerNorm(\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (act): LeakyReLU(negative_slope=0.01)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (out): Linear(\n",
      "    (w): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      ") loading from tmpdir/lm.ckpt, the transferred parameters did not have parameters for the key: rnn.rnn.bias_ih_l0\n",
      "During parameter transfer to RNNLM(\n",
      "  (embedding): Embedding(\n",
      "    (Embedding): Embedding(1000, 256)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (rnn): LSTM(\n",
      "    (rnn): LSTM(256, 2048, num_layers=2, batch_first=True)\n",
      "  )\n",
      "  (dnn): Sequential(\n",
      "    (linear): Linear(\n",
      "      (w): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (norm): LayerNorm(\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (act): LeakyReLU(negative_slope=0.01)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (out): Linear(\n",
      "    (w): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      ") loading from tmpdir/lm.ckpt, the transferred parameters did not have parameters for the key: rnn.rnn.bias_hh_l0\n",
      "During parameter transfer to RNNLM(\n",
      "  (embedding): Embedding(\n",
      "    (Embedding): Embedding(1000, 256)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (rnn): LSTM(\n",
      "    (rnn): LSTM(256, 2048, num_layers=2, batch_first=True)\n",
      "  )\n",
      "  (dnn): Sequential(\n",
      "    (linear): Linear(\n",
      "      (w): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (norm): LayerNorm(\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (act): LeakyReLU(negative_slope=0.01)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (out): Linear(\n",
      "    (w): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      ") loading from tmpdir/lm.ckpt, the transferred parameters did not have parameters for the key: rnn.rnn.weight_ih_l1\n",
      "During parameter transfer to RNNLM(\n",
      "  (embedding): Embedding(\n",
      "    (Embedding): Embedding(1000, 256)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (rnn): LSTM(\n",
      "    (rnn): LSTM(256, 2048, num_layers=2, batch_first=True)\n",
      "  )\n",
      "  (dnn): Sequential(\n",
      "    (linear): Linear(\n",
      "      (w): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (norm): LayerNorm(\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (act): LeakyReLU(negative_slope=0.01)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (out): Linear(\n",
      "    (w): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      ") loading from tmpdir/lm.ckpt, the transferred parameters did not have parameters for the key: rnn.rnn.weight_hh_l1\n",
      "During parameter transfer to RNNLM(\n",
      "  (embedding): Embedding(\n",
      "    (Embedding): Embedding(1000, 256)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (rnn): LSTM(\n",
      "    (rnn): LSTM(256, 2048, num_layers=2, batch_first=True)\n",
      "  )\n",
      "  (dnn): Sequential(\n",
      "    (linear): Linear(\n",
      "      (w): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (norm): LayerNorm(\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (act): LeakyReLU(negative_slope=0.01)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (out): Linear(\n",
      "    (w): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      ") loading from tmpdir/lm.ckpt, the transferred parameters did not have parameters for the key: rnn.rnn.bias_ih_l1\n",
      "During parameter transfer to RNNLM(\n",
      "  (embedding): Embedding(\n",
      "    (Embedding): Embedding(1000, 256)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (rnn): LSTM(\n",
      "    (rnn): LSTM(256, 2048, num_layers=2, batch_first=True)\n",
      "  )\n",
      "  (dnn): Sequential(\n",
      "    (linear): Linear(\n",
      "      (w): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (norm): LayerNorm(\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (act): LeakyReLU(negative_slope=0.01)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (out): Linear(\n",
      "    (w): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      ") loading from tmpdir/lm.ckpt, the transferred parameters did not have parameters for the key: rnn.rnn.bias_hh_l1\n",
      "During parameter transfer to RNNLM(\n",
      "  (embedding): Embedding(\n",
      "    (Embedding): Embedding(1000, 256)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (rnn): LSTM(\n",
      "    (rnn): LSTM(256, 2048, num_layers=2, batch_first=True)\n",
      "  )\n",
      "  (dnn): Sequential(\n",
      "    (linear): Linear(\n",
      "      (w): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (norm): LayerNorm(\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (act): LeakyReLU(negative_slope=0.01)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (out): Linear(\n",
      "    (w): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      ") loading from tmpdir/lm.ckpt, the transferred parameters did not have parameters for the key: dnn.linear.w.weight\n",
      "During parameter transfer to RNNLM(\n",
      "  (embedding): Embedding(\n",
      "    (Embedding): Embedding(1000, 256)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (rnn): LSTM(\n",
      "    (rnn): LSTM(256, 2048, num_layers=2, batch_first=True)\n",
      "  )\n",
      "  (dnn): Sequential(\n",
      "    (linear): Linear(\n",
      "      (w): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (norm): LayerNorm(\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (act): LeakyReLU(negative_slope=0.01)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (out): Linear(\n",
      "    (w): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      ") loading from tmpdir/lm.ckpt, the transferred parameters did not have parameters for the key: dnn.linear.w.bias\n",
      "During parameter transfer to RNNLM(\n",
      "  (embedding): Embedding(\n",
      "    (Embedding): Embedding(1000, 256)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (rnn): LSTM(\n",
      "    (rnn): LSTM(256, 2048, num_layers=2, batch_first=True)\n",
      "  )\n",
      "  (dnn): Sequential(\n",
      "    (linear): Linear(\n",
      "      (w): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (norm): LayerNorm(\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (act): LeakyReLU(negative_slope=0.01)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (out): Linear(\n",
      "    (w): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      ") loading from tmpdir/lm.ckpt, the transferred parameters did not have parameters for the key: dnn.norm.norm.weight\n",
      "During parameter transfer to RNNLM(\n",
      "  (embedding): Embedding(\n",
      "    (Embedding): Embedding(1000, 256)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (rnn): LSTM(\n",
      "    (rnn): LSTM(256, 2048, num_layers=2, batch_first=True)\n",
      "  )\n",
      "  (dnn): Sequential(\n",
      "    (linear): Linear(\n",
      "      (w): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (norm): LayerNorm(\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (act): LeakyReLU(negative_slope=0.01)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (out): Linear(\n",
      "    (w): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      ") loading from tmpdir/lm.ckpt, the transferred parameters did not have parameters for the key: dnn.norm.norm.bias\n",
      "During parameter transfer to RNNLM(\n",
      "  (embedding): Embedding(\n",
      "    (Embedding): Embedding(1000, 256)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (rnn): LSTM(\n",
      "    (rnn): LSTM(256, 2048, num_layers=2, batch_first=True)\n",
      "  )\n",
      "  (dnn): Sequential(\n",
      "    (linear): Linear(\n",
      "      (w): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (norm): LayerNorm(\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (act): LeakyReLU(negative_slope=0.01)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (out): Linear(\n",
      "    (w): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      ") loading from tmpdir/lm.ckpt, the object could not use the parameters loaded with the key: rnn.weight_ih_l0\n",
      "During parameter transfer to RNNLM(\n",
      "  (embedding): Embedding(\n",
      "    (Embedding): Embedding(1000, 256)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (rnn): LSTM(\n",
      "    (rnn): LSTM(256, 2048, num_layers=2, batch_first=True)\n",
      "  )\n",
      "  (dnn): Sequential(\n",
      "    (linear): Linear(\n",
      "      (w): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (norm): LayerNorm(\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (act): LeakyReLU(negative_slope=0.01)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (out): Linear(\n",
      "    (w): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      ") loading from tmpdir/lm.ckpt, the object could not use the parameters loaded with the key: rnn.weight_hh_l0\n",
      "During parameter transfer to RNNLM(\n",
      "  (embedding): Embedding(\n",
      "    (Embedding): Embedding(1000, 256)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (rnn): LSTM(\n",
      "    (rnn): LSTM(256, 2048, num_layers=2, batch_first=True)\n",
      "  )\n",
      "  (dnn): Sequential(\n",
      "    (linear): Linear(\n",
      "      (w): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (norm): LayerNorm(\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (act): LeakyReLU(negative_slope=0.01)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (out): Linear(\n",
      "    (w): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      ") loading from tmpdir/lm.ckpt, the object could not use the parameters loaded with the key: rnn.bias_ih_l0\n",
      "During parameter transfer to RNNLM(\n",
      "  (embedding): Embedding(\n",
      "    (Embedding): Embedding(1000, 256)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (rnn): LSTM(\n",
      "    (rnn): LSTM(256, 2048, num_layers=2, batch_first=True)\n",
      "  )\n",
      "  (dnn): Sequential(\n",
      "    (linear): Linear(\n",
      "      (w): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (norm): LayerNorm(\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (act): LeakyReLU(negative_slope=0.01)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (out): Linear(\n",
      "    (w): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      ") loading from tmpdir/lm.ckpt, the object could not use the parameters loaded with the key: rnn.bias_hh_l0\n",
      "During parameter transfer to RNNLM(\n",
      "  (embedding): Embedding(\n",
      "    (Embedding): Embedding(1000, 256)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (rnn): LSTM(\n",
      "    (rnn): LSTM(256, 2048, num_layers=2, batch_first=True)\n",
      "  )\n",
      "  (dnn): Sequential(\n",
      "    (linear): Linear(\n",
      "      (w): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (norm): LayerNorm(\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (act): LeakyReLU(negative_slope=0.01)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (out): Linear(\n",
      "    (w): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      ") loading from tmpdir/lm.ckpt, the object could not use the parameters loaded with the key: rnn.weight_ih_l1\n",
      "During parameter transfer to RNNLM(\n",
      "  (embedding): Embedding(\n",
      "    (Embedding): Embedding(1000, 256)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (rnn): LSTM(\n",
      "    (rnn): LSTM(256, 2048, num_layers=2, batch_first=True)\n",
      "  )\n",
      "  (dnn): Sequential(\n",
      "    (linear): Linear(\n",
      "      (w): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (norm): LayerNorm(\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (act): LeakyReLU(negative_slope=0.01)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (out): Linear(\n",
      "    (w): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      ") loading from tmpdir/lm.ckpt, the object could not use the parameters loaded with the key: rnn.weight_hh_l1\n",
      "During parameter transfer to RNNLM(\n",
      "  (embedding): Embedding(\n",
      "    (Embedding): Embedding(1000, 256)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (rnn): LSTM(\n",
      "    (rnn): LSTM(256, 2048, num_layers=2, batch_first=True)\n",
      "  )\n",
      "  (dnn): Sequential(\n",
      "    (linear): Linear(\n",
      "      (w): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (norm): LayerNorm(\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (act): LeakyReLU(negative_slope=0.01)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (out): Linear(\n",
      "    (w): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      ") loading from tmpdir/lm.ckpt, the object could not use the parameters loaded with the key: rnn.bias_ih_l1\n",
      "During parameter transfer to RNNLM(\n",
      "  (embedding): Embedding(\n",
      "    (Embedding): Embedding(1000, 256)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (rnn): LSTM(\n",
      "    (rnn): LSTM(256, 2048, num_layers=2, batch_first=True)\n",
      "  )\n",
      "  (dnn): Sequential(\n",
      "    (linear): Linear(\n",
      "      (w): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    )\n",
      "    (norm): LayerNorm(\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (act): LeakyReLU(negative_slope=0.01)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (out): Linear(\n",
      "    (w): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      ") loading from tmpdir/lm.ckpt, the object could not use the parameters loaded with the key: rnn.bias_hh_l1\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "from speechbrain.inference.ASR import EncoderDecoderASR\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Specify the path to your local model directory\n",
    "local_model_path = \"../results/CRDNN_BPE_960h_LM/2602/save/CKPT+2024-07-01+14-24-57+00\"\n",
    "\n",
    "# Load the pre-trained ASR model from the local directory\n",
    "asr_model = EncoderDecoderASR.from_hparams(source=local_model_path, hparams_file=\"inference.yaml\", savedir=\"tmpdir\", run_opts={\"device\": str(device)})\n",
    "\n",
    "# Function to recognize speech in a segment\n",
    "def recognize_speech(segment_path):\n",
    "    transcription = asr_model.transcribe_file(segment_path)\n",
    "    return transcription\n",
    "\n",
    "# Example usage\n",
    "# Apply ASR on each segment and print the results\n",
    "results = []\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    segment_path = os.path.join(output_dir, f\"segment_{turn.start:.1f}_{turn.end:.1f}.wav\")\n",
    "    # print(segment_path)\n",
    "    transcription = recognize_speech(segment_path)\n",
    "    results.append({\n",
    "        \"start\": turn.start,\n",
    "        \"end\": turn.end,\n",
    "        \"speaker\": speaker,\n",
    "        \"transcription\": transcription\n",
    "    })\n",
    "    # print(f\"Speaker {speaker}: {transcription}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>speaker</th>\n",
       "      <th>transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.030969</td>\n",
       "      <td>7.860969</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>والوقتي أه أنا يعني أنا بقيت أقابلهم بطر بجيله...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.216594</td>\n",
       "      <td>1.043469</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>دلوقت الوقتوقت الوقت الولاقتدلوقت الوقتوقت الوقت</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.498469</td>\n",
       "      <td>6.038469</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>هو ده اللي هو ده اللي هو ده اللي هو ده اللي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.147844</td>\n",
       "      <td>16.720344</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>و الله بطريقة جيلك ف اللي هو مامتي بصيتني كده ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.327844</td>\n",
       "      <td>19.487844</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>يعني مش عندي مصري حضرتك يا رب يا رب يا رب يا ر...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17.564094</td>\n",
       "      <td>36.295344</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>يعني مش هتنفذي حضرتك ياراتها وهي اتكلمت عن اخت...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24.769719</td>\n",
       "      <td>26.980344</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "      <td>&lt;fil&gt; &lt;fil&gt; &lt;fil&gt; &lt;fil&gt; &lt;fil&gt; &lt;fil&gt; &lt;fil&gt; &lt;fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>36.599094</td>\n",
       "      <td>37.307844</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>38.033469</td>\n",
       "      <td>46.977219</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>ودي أقول بصراحه الناس ممكن تقفل الناس ممكن تقو...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       start        end     speaker  \\\n",
       "0   0.030969   7.860969  SPEAKER_00   \n",
       "1   0.216594   1.043469  SPEAKER_01   \n",
       "2   5.498469   6.038469  SPEAKER_01   \n",
       "3   8.147844  16.720344  SPEAKER_00   \n",
       "4  17.327844  19.487844  SPEAKER_00   \n",
       "5  17.564094  36.295344  SPEAKER_01   \n",
       "6  24.769719  26.980344  SPEAKER_02   \n",
       "7  36.599094  37.307844  SPEAKER_01   \n",
       "8  38.033469  46.977219  SPEAKER_01   \n",
       "\n",
       "                                       transcription  \n",
       "0  والوقتي أه أنا يعني أنا بقيت أقابلهم بطر بجيله...  \n",
       "1   دلوقت الوقتوقت الوقت الولاقتدلوقت الوقتوقت الوقت  \n",
       "2        هو ده اللي هو ده اللي هو ده اللي هو ده اللي  \n",
       "3  و الله بطريقة جيلك ف اللي هو مامتي بصيتني كده ...  \n",
       "4  يعني مش عندي مصري حضرتك يا رب يا رب يا رب يا ر...  \n",
       "5  يعني مش هتنفذي حضرتك ياراتها وهي اتكلمت عن اخت...  \n",
       "6  <fil> <fil> <fil> <fil> <fil> <fil> <fil> <fil...  \n",
       "7                                                     \n",
       "8  ودي أقول بصراحه الناس ممكن تقفل الناس ممكن تقو...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
